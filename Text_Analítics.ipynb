{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliotecas"
      ],
      "metadata": {
        "id": "fQYm9o67ew4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalação\n",
        "\n"
      ],
      "metadata": {
        "id": "Of-8w80TekrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Modelo de PLN Spacy para o idioma Português"
      ],
      "metadata": {
        "id": "d4iYr405hFY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "RF8iX67M8QO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Biblioteca para Manipulação Planilhas de Excel"
      ],
      "metadata": {
        "id": "5CNyC9SesfyB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "98grrCmQ_Le2"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importação"
      ],
      "metadata": {
        "id": "ev4wTI3se2Ut"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "eTVjmHKL_Ojg"
      },
      "outputs": [],
      "source": [
        "# Manipulação df\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Manipulação texto\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# Contagem de tokens\n",
        "from collections import Counter\n",
        "\n",
        "# Modelo Manipulação PLN\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Modelo PLN\n",
        "import spacy\n",
        "from spacy.lang.pt import Portuguese\n",
        "\n",
        "# Manipulação de Arquivos no Colab\n",
        "from google.colab import files\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload"
      ],
      "metadata": {
        "id": "5kZCih_GhYDp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "B9pG2MEd_aDv"
      },
      "outputs": [],
      "source": [
        "df = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fluxo de Tratamento (EXEC)"
      ],
      "metadata": {
        "id": "RS0hF62m5yJN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "oabMt8Y2DAdm"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('MOTIVOS.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Limpeza de texto com regex:\n",
        "\n",
        "  - No sistema, o campo observações possui escrita livre. Portanto, não existe nenhum para padrão que possa ser filtrado.\n",
        "\n",
        "  - Foi observado que, geralmente, o principal motivo é descrito nas primeiras duas frases.\n",
        "\n",
        "  - Dentro do que foi observado, utilizamos regex para realizar a primeira etapa limpeza e padronização do texto."
      ],
      "metadata": {
        "id": "XSV75DrEBAng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Limpeza_Regex(texto):\n",
        "  # tira acentos\n",
        "  texto = unicodedata.normalize('NFD', texto)\n",
        "  texto = texto.encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "  # lower case\n",
        "  texto = texto.lower()\n",
        "\n",
        "  # remove todos os asteriscos\n",
        "  texto = re.sub(r'\\*+', '', texto)\n",
        "\n",
        "  # remove todos os '>'\n",
        "  texto = re.sub(r'\\>+', '', texto)\n",
        "\n",
        "  # remove quebras de linha do início\n",
        "  texto = re.sub(r'^(\\s*(_x000d_|\\n))+', '', texto)\n",
        "\n",
        "    # tratamento especial para o padrão \"_x000D_\\n_x000D_\\n\"\n",
        "  if re.search(r'_x000d_\\s*_x000d_\\s*', texto):\n",
        "      # remove o \"_x000d_\" e espaços extras\n",
        "      texto = re.sub(r'_x000d_\\s*_x000d_\\s*', ' ', texto)\n",
        "      # mantém só até a primeira quebra de linha depois disso\n",
        "      texto = texto.split('\\n')[0]\n",
        "  else:\n",
        "      # separa por quebras de linha normalmente\n",
        "      texto = re.split(r'\\n', texto, maxsplit=2)\n",
        "      texto = '\\n'.join(texto[:2])\n",
        "\n",
        "  # remove códigos tipo _x000d_\n",
        "  texto = re.sub(r'_x000d_', '', texto)\n",
        "\n",
        "  # remove quebras de linha no início e no fim\n",
        "  texto = texto.strip()\n",
        "\n",
        "  # remove espaços extras\n",
        "  texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "\n",
        "  return texto\n"
      ],
      "metadata": {
        "id": "nrO0mnBb8xlT"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "ssEcEg6zGfzf"
      },
      "outputs": [],
      "source": [
        "df['Limpeza_Regex'] = df['obs'].fillna('').apply(lambda x: Limpeza_Regex(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Stop Words\n",
        "\n",
        "  -   Objeto com lista de que não possuem significado na frase (stop_words)\n"
      ],
      "metadata": {
        "id": "WpsZZu5GrESB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = spacy.lang.pt.stop_words.STOP_WORDS"
      ],
      "metadata": {
        "id": "PCrQzB2DBmfJ"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def no_stopwords(texto):\n",
        "  doc = nlp_pt(texto)\n",
        "  return \" \".join(x.text for x in doc if x.text not in stop_words)"
      ],
      "metadata": {
        "id": "hnxKttjIBrQJ"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lemmatization\n",
        "\n",
        "  - Retorna as palavras a sua forma base, respeitando gramática e contexto"
      ],
      "metadata": {
        "id": "bihAUzp_6-q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# carrega modelo pré treinado Spacy para Português\n",
        "nlp_pt = spacy.load('pt_core_news_sm')"
      ],
      "metadata": {
        "id": "FoJitYgorErn"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(texto):\n",
        "  doc = nlp_pt(texto)\n",
        "  return ' '.join([token.lemma_ for token in doc])"
      ],
      "metadata": {
        "id": "yaGy5q9e56Ou"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Applying"
      ],
      "metadata": {
        "id": "keCMidBytPnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['no_stopwords'] = df['Limpeza_Regex'].apply( lambda x: no_stopwords(x))"
      ],
      "metadata": {
        "id": "v26AUpxVBx8B",
        "collapsed": true
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['lemmatization'] = df['no_stopwords'].apply( lambda x: lemmatization(x))"
      ],
      "metadata": {
        "id": "IEMF8BH57Xi3"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Segunda limpeza\n",
        "\n",
        "  - tirando palavras frequentes que não agregam significado, acentos e excesso de espaços."
      ],
      "metadata": {
        "id": "XJablAXeLCG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Limpeza_Regex2(texto):\n",
        "\n",
        "  # excluir palavra enviar\n",
        "  texto = re.sub(r'enviar', '', texto)\n",
        "\n",
        "  # excluir word 'anexar'\n",
        "  texto = re.sub(r'anexar', '', texto)\n",
        "\n",
        "  # excluir word 'prezar'\n",
        "  texto = re.sub(r'prezar', '', texto)\n",
        "\n",
        "  # excluir simbolo '/'\n",
        "  texto = re.sub(r'/', '', texto)\n",
        "\n",
        "  # excluir simbolo ':'\n",
        "  texto = re.sub(r':', '', texto)\n",
        "\n",
        "  # excluir simbolo ','\n",
        "  texto = re.sub(r',', '', texto)\n",
        "\n",
        "  # remover espaços duplos ou mais\n",
        "  texto = re.sub(r'\\s{2,}', ' ', texto)\n",
        "\n",
        "  # substituir '-'\n",
        "  texto = re.sub(r'-', ' ', texto)\n",
        "\n",
        "  return texto"
      ],
      "metadata": {
        "id": "FmqWKgrDf95L"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Limpeza_Regex2'] = df['lemmatization'].apply( lambda x: Limpeza_Regex2(x))"
      ],
      "metadata": {
        "id": "Ir3UZo-Ziylb"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estatísticas de texto"
      ],
      "metadata": {
        "id": "IqpLE2uuTGwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Frequencia de frase texto:\n",
        "\n",
        "  - Quantas vezes repetiu texto inteiro com o mesmo valor"
      ],
      "metadata": {
        "id": "W_sWNze1OreG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequencia de tokens com N-Grams"
      ],
      "metadata": {
        "id": "e4ENojA3GUFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Processamento de tokens"
      ],
      "metadata": {
        "id": "GRMQkpn7W0uR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = \" \".join(df['Limpeza_Regex2'])"
      ],
      "metadata": {
        "id": "j2KWaCF_GWCU"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tokenizador pré treinado do nltk"
      ],
      "metadata": {
        "id": "F8lyFoPFTy1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "GXymrv5QHI81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Separa o texto em tokens"
      ],
      "metadata": {
        "id": "ycdStkrWWj_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(words)"
      ],
      "metadata": {
        "id": "-Iymq7W3Gvse"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Unigrams"
      ],
      "metadata": {
        "id": "uogtF0ipT8fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams = nltk.ngrams(tokens, 1)"
      ],
      "metadata": {
        "id": "qHzoaKAxT78a"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(unigrams).most_common(50))"
      ],
      "metadata": {
        "id": "WwW8Yg4HRhIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Bigrams"
      ],
      "metadata": {
        "id": "lbgeTGkzSmt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = nltk.ngrams(tokens, 2)"
      ],
      "metadata": {
        "id": "sc59nkwdSoDr"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(bigrams).most_common(50))"
      ],
      "metadata": {
        "id": "CTTrt8WeSuP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Concatenar frases:\n",
        "\n",
        "  - Concatenar frase para valer como tokens únicos"
      ],
      "metadata": {
        "id": "YQh6Bs90SAbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frases(texto):\n",
        "\n",
        "\n",
        "  # frase 1\n",
        "  texto = re.sub(r'alteracao valor', 'alteracao_valor', texto)\n",
        "  # frase 2\n",
        "  texto = re.sub(r'confirmar valor', 'confirmar_valor', texto)\n",
        "  # frase 3\n",
        "  texto = re.sub(r'documento identificacao', 'documento_identificacao', texto)\n",
        "  # frase 4\n",
        "  texto = re.sub(r'3 ultimo extrato', '3_ultimo_extrato', texto)\n",
        "  # frase 5\n",
        "  texto = re.sub(r'video ok', '', texto)\n",
        "  # frase 6\n",
        "  texto = re.sub(r'selfie legivel', '', texto)\n",
        "\n",
        "  return texto"
      ],
      "metadata": {
        "id": "GRmBxpHFOIfR"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Limpeza_Regex2'] = df['Limpeza_Regex2'].apply( lambda x: frases(x))"
      ],
      "metadata": {
        "id": "GZJiEW89cQsQ"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Listar Palavras Chave para cada Tópico"
      ],
      "metadata": {
        "id": "wbsTvgK4adsV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "onjXOw7k8vmp"
      },
      "outputs": [],
      "source": [
        "palavras_topicos = {\n",
        "\n",
        "    'SELFIE': ['selfie'],\n",
        "    'ALTERAÇÃO DE VALORES': ['alteracao_valor','confirmar_valor','alteracao', 'alteracoes'],\n",
        "    'VÍDEO/ÁUDIO PENDENTE': ['script','inaudivel','audio','gravacao','video'],\n",
        "    'DOCUMENTOS': ['documento_identificacao','3_ultimo_extrato','contrato','extrato','comprovante','documento','detalhamento','print','copia','autorizar','pdf','pdfer','legivel','extrato']}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Classificação de texto simples baseada em Palavras Chave"
      ],
      "metadata": {
        "id": "03aln-Wkaj0U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "CWwITNcUHrE0"
      },
      "outputs": [],
      "source": [
        "def atribuir_topico_com_palavras_chave(texto, palavras_topicos):\n",
        "    for i, (topico, palavras) in enumerate(palavras_topicos.items()):\n",
        "        if any(palavra in texto for palavra in palavras):\n",
        "            return topico  # Retorna o tópico\n",
        "    return  'DOCUMENTOS'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar coluna de rótulo\n",
        "df[\"rótulo_tópico\"] = df[\"Limpeza_Regex2\"].apply(lambda x: atribuir_topico_com_palavras_chave(x, palavras_topicos))"
      ],
      "metadata": {
        "id": "K53iX2vGcqo8"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* DF para Excel"
      ],
      "metadata": {
        "id": "FfkLSV3xi-GC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "1pd1STs6Iq49"
      },
      "outputs": [],
      "source": [
        "df.to_excel('Pendencias.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Download"
      ],
      "metadata": {
        "id": "9IMiY7smjCVz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "s3utRkjSKobN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "efcf7cba-b18e-4209-f336-a84b5cd7ecc5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_740cc38f-7eb9-4a9c-b5a0-aa3787da4b9a\", \"Pendencias.xlsx\", 3450212)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "files.download('Pendencias.xlsx')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
